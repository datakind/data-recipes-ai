import asyncio
import datetime
import glob
import json
import os
import sys
import zipfile

import pandas as pd
import requests
from dotenv import load_dotenv
from jinja2 import Environment, FileSystemLoader
from openai import AzureOpenAI, OpenAI

load_dotenv("../../.env")

api_key = os.environ.get("ASSISTANTS_API_KEY")
assistant_id = os.environ.get("ASSISTANTS_ID")
model = os.environ.get("ASSISTANTS_MODEL")
api_type = os.environ.get("ASSISTANTS_API_TYPE")
api_endpoint = os.environ.get("ASSISTANTS_BASE_URL")
api_version = os.environ.get("ASSISTANTS_API_VERSION")
bot_name = os.environ.get("ASSISTANTS_BOT_NAME")
environment = Environment(loader=FileSystemLoader("templates/"))

file_to_func_map_loc = "./file_to_func_map.json"
data_files_location = "../../ingestion/api"

# Needed to get common fields standard_names
INTEGRATION_CONFIG = "../../ingestion/ingestion.config"
DATASET_DETAILS = "dataset_details.json"
SYSTEM_PROMPT = "instructions.txt"

if api_type == "openai":
    print("Using OpenAI API")
    client = OpenAI(api_key=api_key)
elif api_type == "azure":
    print("Using Azure API")
    print(f"Endpoint: {api_endpoint}")
    client = AzureOpenAI(
        api_key=api_key, api_version=api_version, azure_endpoint=api_endpoint
    )
else:
    print("API type not supported")
    sys.exit(1)


def get_common_field_standard_names():
    """
    Get the standard names of common fields from the integration configuration file.

    Returns:
        list: A list of standard names of common fields.
    """
    with open(INTEGRATION_CONFIG) as f:
        print(f"Reading {INTEGRATION_CONFIG}")
        config = json.load(f)
    return config["standard_names"]


def get_manually_defined_functions():
    """
    Get a list of manually defined functions.

    Returns:
        list: A list of dictionaries representing the manually defined functions.
    """
    # functions = [
    #     {
    #         "function": {
    #             "name": "get_info_about_datasets",
    #             "parameters": {},
    #             "description": """
    #                 Get a JSON object containing information about the datasets you have access to.
    #                 This includes which types of data, the countries they include and columns within each datafiles.
    #                 Use this function for questions about the data you have
    #             """,
    #         }
    #     }
    # ]
    functions = []
    if len(functions) > 0:
        functions_openai_fmt = []
        for f in functions:
            f = {
                "type": "function",
                "function": f["function"],
            }
            functions_openai_fmt.append(f)
    return functions_openai_fmt


def upload_files_to_openai(standard_names):
    """
    Uploads files to OpenAI and returns a prompt string and a list of file IDs.

    Args:
        standard_names (dict): A dictionary containing common field standard_names.

    Returns:
        file_prompt (str): A string containing information about the uploaded files.
        file_ids (list): A list of file IDs generated by OpenAI.
    """

    files = glob.glob(f"{data_files_location}/**/*.csv", recursive=True)
    files += glob.glob(f"{data_files_location}/**/*geoBoundaries*.zip", recursive=True)
    file_prompt = ""
    file_ids = []

    # sort files with csv first, then zip
    files = sorted(files, key=lambda x: x.split(".")[-1])

    datafiles = []
    for f in files:
        print(f)
        countries = ""
        first_line = ""
        # Get column standard_names from first line
        if f.endswith(".csv"):
            df = pd.read_csv(f)
            first_line = list(df.columns)
            if standard_names["country_code_field"] in first_line:
                countries = list(df[standard_names["country_code_field"]].unique())

        print(f"Uploading {f} ...")
        file = client.files.create(file=open(f, "rb"), purpose="assistants")

        r = {}
        if f.endswith(".csv"):
            file_loc = f"/mnt/data/{file.id}"
            r["file_location"] = file_loc
            r["_original_file_name"] = f.split("/")[-1]
            metadata_file = f.replace(".csv", "_meta.json")
            r["description"] = "This is CSV data"

            # If we have a metadata file, use that
            if os.path.exists(metadata_file):
                with open(metadata_file) as mf:
                    metadata = json.load(mf)
                    description = ""
                    for f in ["tags", "summary", "description"]:
                        if f in metadata["get"]:
                            description += str(metadata["get"][f]) + "\n"
                    r["description"] = description

            r["columns"] = first_line
            r["countries"] = countries
        elif "geoBoundaries" in f:
            r["zip_file_location_with_shapefiles"] = f"/mnt/data/{file.id}"
            r["_original_file_name"] = f
            r["description"] = (
                "This file contains administrative boundary data for countries and admin level as specified"
            )
            r["admin_level"] = f.split("geoBoundaries-")[1][0:4]
            # Intentionall removed some columns here for clarity
            r["columns"] = [
                "Shape_Leng",
                "Shape_Area",
                f"{standard_names['admin0_code_field']}",
                f"{standard_names['admin1_code_field']}",
                f"{standard_names['admin2_code_field']}",
                f"{standard_names['admin3_code_field']}",
                "ADM1_REF",
                "date",
                "validOn",
                "validTo",
                "geometry",
            ]

            with zipfile.ZipFile(f, "r") as zip_ref:
                shape_files = []
                files_in_zip = zip_ref.namelist()
                for zf in files_in_zip:
                    if zf.endswith(".shp"):
                        r2 = {}
                        r2["shape_file"] = zf
                        r2["country"] = zf[0:3].upper()
                        shape_files.append(r2)

            r["shapefiles"] = shape_files

        datafiles.append(r)
        print(json.dumps(datafiles, indent=4))

        file_ids.append(file.id)

    file_prompt = json.dumps(datafiles, indent=4)

    return file_prompt, file_ids


def create_update_assistant():
    """
    Creates or updates a humanitarian response assistant.

    To force creation of a new assistant, be sure that ASSITANT_ID is not set in the .env file.

    """

    standard_names = get_common_field_standard_names()

    instructions = f"""
        "You are a helpful humanitarian response analyst. You answer data-related questions using only the data sources provided in your functions"

        "You only answer questions about humanitarian data, nothing else"

        "Never, ever use sample data, always use real data from the files or functions provided"

        "When plotting numerical scales don't use scientific notation, use thousands, millions, billions etc"

        "Here is the mapping column for locations between tabular datasets and shapefiles:
            administrative levels 0 : {standard_names['admin0_code_field']}
            administrative levels 1 : {standard_names['admin1_code_field']}
            administrative levels 2 : {standard_names['admin2_code_field']}
            administrative levels 3 : {standard_names['admin3_code_field']}"

        "You have been provided files to analyze, these are found '/mnt/data/<FILE ID>'."

        "You do not need to add a suffix like '.csv' or .zip' when reading the files provided"

        "You do not output your analysis plan, just the answer"

        "If asked what data you have, list the data you have but don't provide file standard_names or IDs. Do provide the type of data though, eg population"

        "Add tabular data is from the humanitarian data exchange (HDX) new HAPI API"

        "ALWAYS filter tabular data by code variables, not standard_names. So for example {standard_names['admin0_code_field']} for country, {standard_names['admin1_code_field']} for admin level 1 etc"

        "Gender columns are set to 'm' or 'f' if set"

        "When generating code, define all files and folders as variables at the top of your code, then reference in code below"

        "Always make sure the variable for the folder name to extract zip files is different to variable for the location of the zip file"

        "ALWAYS Import the following modules in generated code: pandas, geopandas, matplotlib.pyplot, zipfile, os"

        "If asked to display a table, use the 'display' command in python"

        "Always display generated images inline, NEVER give a link to the image or map"

        "If you generate code, run it"

        "If a dataset has admin standard_names in it, no need to merge with administrative data"

    """

    tools = [{"type": "code_interpreter"}]

    file_prompt, file_ids = upload_files_to_openai(standard_names)

    # Save file_prompt
    with open(DATASET_DETAILS, "w") as f:
        f.write(file_prompt)

    instructions += (
        "\n\n===============\n\nThese are the data files you have access to:\n\n"
        + "\n\nDATA FILES:\n\n"
        + file_prompt
    )
    instructions += "\n\nBoundary shape files needed for maps can be found in the provided zip files of format geoBoundaries-adm1-countries_a-z.zip\n"
    instructions += "The file standard_names indicate what country and admin level they relate too, eg 'ukr_admbnda_adm1.shp' where 'ukr' is Ukraine and adm1 indicates admin level 1"
    instructions += "The unzipped shapefiles have country code in the first 3 letters of their name, eg ukr_admbnda_adm1.shp (the date part can change depending on country)\n"
    instructions += "Only use boundary zip files if you have been explicitly asked to plot on a map. No need to use for other plots\n"
    instructions += f"When merging shapefiles with HDX datafiles, use columns {standard_names['admin0_code_field']} for admin 0, {standard_names['admin1_code_field']} for admin level 1 and {standard_names['admin2_code_field']} for admin level 2"

    # Load code examples
    template = environment.get_template("sample_code.jinja")
    sample_code = template.render(admin1_code_name=standard_names["country_code_field"])
    instructions += "\n\n======= SAMPLE CODE ========\n\n" + sample_code

    print(instructions)
    print(api_type)
    print(file_ids)

    with open(SYSTEM_PROMPT, "w") as f:
        f.write(instructions)

    # Find if agent exists
    try:
        print(
            f"Updating existing assistant {assistant_id} {bot_name} and model {model} ..."
        )
        assistant = client.beta.assistants.update(
            assistant_id,
            name=bot_name,
            instructions=instructions,
            tools=tools,
            model=model,
            file_ids=file_ids,
        )
    except Exception:
        print(f"Creating assistant with model {model} ...")
        assistant = client.beta.assistants.create(
            name=bot_name,
            instructions=instructions,
            tools=tools,
            model=model,
            file_ids=file_ids,
        )
        print(assistant)
        print("Now save the ID in your .env file")
        # SAVE IT IN .env


if __name__ == "__main__":
    create_update_assistant()
